import gymnasium as gym
import copy
import math
import numpy as np
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Classe Node : chaque nÅ“ud contient une copie de l'env
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Node:
    def __init__(self, env, parent=None):
        self.env = env                      # copie de lâ€™environnement (sans rendu)
        self.parent = parent
        self.children = {}                  # action -> Node
        self.visits = 0
        self.reward = 0.0
        self.depth = 0 if parent is None else parent.depth + 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Fonctions MCTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fully_expanded(node):
    return len(node.children) == node.env.action_space.n

def expand(node):
    for action in range(node.env.action_space.n):
        if action not in node.children:
            env_copy = copy.deepcopy(node.env)
            env_copy.reset()  # ğŸ”§ NÃ©cessaire pour autoriser .step()
            obs, reward, terminated, truncated, _ = env_copy.step(action)
            child = Node(env_copy, parent=node)
            node.children[action] = child
            return child
    return None

def best_uct(node, c=1.4):
    def uct(child):
        if child.visits == 0:
            return float("inf")
        exploitation = child.reward / child.visits
        exploration = c * math.sqrt(math.log(node.visits + 1) / child.visits)
        return exploitation + exploration
    return max(node.children.values(), key=uct)

def rollout(node):
    env_sim = copy.deepcopy(node.env)
    env_sim.reset()  # ğŸ”§ NÃ©cessaire pour autoriser .step()
    done = False
    total_reward = 0
    while not done:
        action = env_sim.action_space.sample()
        _, reward, terminated, truncated, _ = env_sim.step(action)
        total_reward = reward  # Reward final (Blackjack)
        done = terminated or truncated
    return total_reward

def backpropagate(node, reward):
    while node:
        node.visits += 1
        node.reward += reward
        node = node.parent

def monte_carlo_tree_search(root, make_env, simulations=1000, c=1.4, verbose=False):
    all_rewards = []
    max_depth = 0

    for _ in range(simulations):
        node = root
        while fully_expanded(node):
            node = best_uct(node, c)
        leaf = expand(node) or node
        reward = rollout(leaf)
        backpropagate(leaf, reward)

        all_rewards.append(reward)
        max_depth = max(max_depth, leaf.depth)

    best_action = max(root.children, key=lambda a: root.children[a].visits)

    if verbose:
        print(f"\nğŸ¯ Action choisie par MCTS : {best_action} ('{'stick' if best_action == 0 else 'hit'}')")
        for a, child in root.children.items():
            avg_r = child.reward / child.visits if child.visits else 0
            print(f"  Action {a}: visits={child.visits:4}, avg_reward={avg_r:+.3f}")
        print(f"ğŸ“Š Simulations: {simulations},  Avg rollout reward={np.mean(all_rewards):+.3f},  Profondeur max={max_depth}")

    return best_action

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Fonction pour crÃ©er des environnements
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_env(render=False):
    return gym.make("Blackjack-v1", render_mode="human" if render else None)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Boucle de jeu principale avec rendu
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env_main = make_env(render=True)
obs, _ = env_main.reset()

# Racine basÃ©e sur une copie silencieuse
root = Node(copy.deepcopy(make_env()))

done = False
total_reward = 0
step_idx = 0

print("\nğŸš€ DÃ©but de la partie Blackjack avec MCTS")

while not done:
    action = monte_carlo_tree_search(
        root,
        make_env=make_env,
        simulations=1000,
        verbose=True
    )

    obs, reward, terminated, truncated, _ = env_main.step(action)
    done = terminated or truncated
    step_idx += 1

    print(f"\nâœ… Tour {step_idx} â€” Action jouÃ©e: {'stick' if action == 0 else 'hit'}")
    if done:
        print(f"ğŸ Partie terminÃ©e â€” Reward final: {reward:+}")
        total_reward = reward
    else:
        print(f"   Jeu continueâ€¦")

    # Avancer dans l'arbre ou redÃ©marrer
    if action in root.children:
        root = root.children[action]
        root.parent = None
    else:
        root = Node(copy.deepcopy(make_env()))

env_main.close()
print(f"\nğŸ‰ Score total (gain par partie) : {total_reward:+}\n")
